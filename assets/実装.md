# 実装

通常のfine-tuningに加えて、以下の処理を追加した`adarand.py`を実装した。

```shell
$ diff fine_tuning.py adarand.py

# 抜粋

# 以下を用いる
#   - 出力と特徴量をどちらも返すResNet50のカスタムモデル
#   - 混合ガウス分布による特徴空間におけるランダムなベクトルをサンプリングするclass
#   - 混合ガウス分布の平均を更新するための損失関数
# 詳細はmodels.pyを参照
7a8
> from models import CustomResNet50, RandomFeatureSampler, adarand_loss
#   - モデル更新の損失関数に追加される正則化項の係数
#   - 混合ガウス分布の平均を更新する確率的勾配法の学習率
21a23,24
>     reg_lambda: float = 1.0
>     rand_lr: float = 1e-3

# 混合ガウス分布の平均・分散を事前学習モデルのtarget datasetに対する特徴ベクトルから計算する
>     resnet50 = CustomResNet50(num_classes=len(test_dataset.classes))
63a65,87
>     precomputed_sum1 = torch.zeros(len(test_dataset.classes), resnet50.module.resnet50.fc.in_features).to(device)
>     precomputed_sum2 = torch.zeros(len(test_dataset.classes), resnet50.module.resnet50.fc.in_features).to(device)
>     class_nums = torch.zeros(len(test_dataset.classes)).to(device)
>     with torch.no_grad():
>         for x, y in tqdm(train_loader, total=len(train_loader), desc="Precomputing mu and sigma"):
>             x, y = x.to(device), y.to(device)
>             _, feat = resnet50(x)
>             y_onehot = torch.nn.functional.one_hot(y, len(test_dataset.classes)).float()
>             precomputed_sum1 += y_onehot.T @ feat
>             precomputed_sum2 += y_onehot.T @ (feat**2)
>             class_nums += y_onehot.sum(dim=0)
> 
>     mu = precomputed_sum1 / class_nums.view(-1, 1)
>     sigma = torch.sqrt((precomputed_sum2 / class_nums.view(-1, 1)) - mu**2)
> 
>     random_features = RandomFeatureSampler(
>         feat_dim=resnet50.module.resnet50.fc.in_features,
>         num_classes=len(test_dataset.classes),
>         mu=mu,
>         sigma=sigma,
>     )
>     random_features.to(device)
> 

# 混合ガウス分布の平均を確率的勾配法で更新する
70a95,97
>     rand_optimizer = torch.optim.Adam(random_features.parameters(), lr=config.rand_lr)
>     rand_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(rand_optimizer, T_max=config.max_iter)
> 

# 正則化項が追加された損失を用いてモデルのパラメータを更新する
75a103,104
>         running_loss_ce = 0.0
>         running_loss_reg = 0.0
81,82c110,113
>             y_pred, feat = resnet50(x)
>             loss_ce = torch.nn.functional.cross_entropy(y_pred, y)
>             loss_reg = torch.nn.functional.mse_loss(random_features(y), feat)
>             loss = loss_ce + config.reg_lambda * loss_reg
86a118,119
>             running_loss_ce += loss_ce.item()
>             running_loss_reg += loss_reg.item()
88a122,126
>             rand_optimizer.zero_grad()
>             rand_loss = adarand_loss(random_features, feat.clone().detach(), y)
>             rand_loss.backward()
>             rand_optimizer.step()
> 
89a128
>         rand_scheduler.step()
98c137
>                 y_pred, _ = resnet50(x)
109c148
>         epoch_result = f"Epoch {epoch}/{config.max_iter}, Loss: {running_loss / train_size}, CE: {running_loss_ce / train_size}, Reg: {running_loss_reg / train_size}, Top-1 Acc: {top1_acc}, Top-5 Acc: {top5_acc}"
```
